{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curve fitting\n",
    "=============\n",
    "\n",
    "Curve fitting in ERLabPy largely relies on [lmfit](https://lmfit.github.io/lmfit-py/). Along with some convenient models for common fitting tasks, ERLabPy provides a powerful accessor that streamlines curve fitting on multidimensional xarray objects.\n",
    "\n",
    "ERLabPy also provides optional integration of lmfit models with [iminuit ](https://github.com/scikit-hep/iminuit), which is a Python interface to the [Minuit C++ library ](https://root.cern.ch/doc/master/Minuit2Page.html) developed at CERN.\n",
    "\n",
    "In this tutorial, we begin with some convenient functions that ERLabPy provides for common tasks such as Fermi edge fitting. Next, we will start with the basics of curve fitting using lmfit, introduce some models that are available in ERLabPy, and demonstrate curve fitting with the {meth}`modelfit <erlab.accessors.fit.ModelFitDataArrayAccessor.__call__>` accessor to fit multidimensional xarray objects. Finally, we will show how to use [iminuit](https://github.com/scikit-hep/iminuit) with lmfit models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import erlab.plotting as eplt\n",
    "import erlab.analysis as era\n",
    "from erlab.io.exampledata import generate_gold_edge\n",
    "\n",
    "plt.rcParams[\"figure.constrained_layout.use\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(fermi edge fitting)=\n",
    "\n",
    "## Fermi edge fitting\n",
    "\n",
    "Functions related to the Fermi edge are available in {mod}`erlab.analysis.gold`. To fit a polynomial to a Fermi edge, you can use {func}`erlab.analysis.gold.poly`.\n",
    "\n",
    "\n",
    ":::{hint}\n",
    "\n",
    "The interactive Fermi edge fitting tool {func}`erlab.interactive.goldtool` can be used to generate the code below interactively.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = generate_gold_edge(temp=100, seed=1)\n",
    "\n",
    "result = era.gold.poly(\n",
    "    gold,\n",
    "    angle_range=(-15, 15),\n",
    "    eV_range=(-0.2, 0.2),\n",
    "    temp=100.0,\n",
    "    vary_temp=False,\n",
    "    degree=2,\n",
    "    plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting polynomial can be used to correct the Fermi edge with {func}`erlab.analysis.gold.correct_with_edge`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era.gold.correct_with_edge(gold, result).qplot(cmap=\"Greys\")\n",
    "eplt.fermiline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve fitting with `lmfit`\n",
    "\n",
    "In this section, we will cover basic curve fitting using [lmfit](https://lmfit.github.io/lmfit-py/). If you are familiar with the library, you can [skip to the next section](pre-defined-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import erlab.analysis as era\n",
    "import erlab.plotting as eplt\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = [\"svg\", \"pdf\"]\n",
    "plt.rcParams[\"figure.dpi\"] = 96\n",
    "plt.rcParams[\"image.cmap\"] = \"viridis\"\n",
    "plt.rcParams[\"figure.figsize\"] = eplt.figwh(wscale=1.2, fixed_height=False)\n",
    "\n",
    "nb_execution_mode = \"cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining a model function and the data to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly1(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "\n",
    "# Generate some toy data\n",
    "x = np.linspace(0, 10, 20)\n",
    "y = poly1(x, 1, 2)\n",
    "\n",
    "# Add some noise with fixed seed for reproducibility\n",
    "rng = np.random.default_rng(1)\n",
    "yerr = np.full_like(x, 0.5)\n",
    "y = rng.normal(y, yerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a simple function\n",
    "\n",
    "A lmfit model can be created by calling {class}`lmfit.Model` class with the model function and the independent variable(s) as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmfit\n",
    "\n",
    "model = lmfit.Model(poly1)\n",
    "params = model.make_params(a=1.0, b=2.0)\n",
    "result = model.fit(y, x=x, params=params, weights=1 / yerr)\n",
    "\n",
    "result.plot()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By passing dictionaries to `make_params`, we can set the initial values of the parameters and also set the bounds for the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lmfit.Model(poly1)\n",
    "params = model.make_params(\n",
    "    a={\"value\": 1.0, \"min\": 0.0},\n",
    "    b={\"value\": 2.0, \"vary\": False},\n",
    ")\n",
    "result = model.fit(y, x=x, params=params, weights=1 / yerr)\n",
    "_ = result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`result` is a {class}`lmfit.model.ModelResult` object that contains the results of the fit. The best-fit parameters can be accessed through the `result.params` attribute.\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Since all weights are the same in this case, it has little effect on the fit results. However, if we are confident that we have a good estimate of `yerr`, we can pass `scale_covar=True` to the `fit` method to obtain accurate uncertainties.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.params[\"a\"].value, result.params[\"a\"].stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters can also be retrieved in a form that allows easy error propagation calculation, enabled by the [uncertainties](https://github.com/lmfit/uncertainties) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_uvar = result.uvars[\"a\"]\n",
    "print(a_uvar)\n",
    "print(a_uvar**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting with composite models\n",
    "\n",
    "Before fitting, let us generate a Gaussian peak on a linear background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate toy data\n",
    "x = np.linspace(0, 10, 50)\n",
    "y = -0.1 * x + 2 + 3 * np.exp(-((x - 5) ** 2) / (2 * 1**2))\n",
    "\n",
    "# Add some noise with fixed seed for reproducibility\n",
    "rng = np.random.default_rng(5)\n",
    "yerr = np.full_like(x, 0.3)\n",
    "y = rng.normal(y, yerr)\n",
    "\n",
    "# Plot the data\n",
    "plt.errorbar(x, y, yerr, fmt=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A composite model can be created by adding multiple models together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmfit.models import GaussianModel, LinearModel\n",
    "\n",
    "model = GaussianModel() + LinearModel()\n",
    "params = model.make_params(slope=-0.1, center=5.0, sigma={\"value\": 0.1, \"min\": 0})\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.fit(y, x=x, params=params, weights=1 / yerr)\n",
    "result.plot()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about multiple gaussian peaks? Since the parameter names overlap between the models, we must use the `prefix` argument to distinguish between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianModel(prefix=\"p0_\") + GaussianModel(prefix=\"p1_\") + LinearModel()\n",
    "model.make_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, see the [lmfit documentation](https://lmfit.github.io/lmfit-py/fitting.html).\n",
    "\n",
    "(pre-defined-models)=\n",
    "\n",
    "## Fitting with pre-defined models\n",
    "\n",
    "Creating composite models with different prefixes every time can be cumbersome, so ERLabPy provides some pre-defined models in {mod}`erlab.analysis.fit.models`.\n",
    "\n",
    "\n",
    "### Fitting multiple peaks\n",
    "\n",
    "One example is {class}`MultiPeakModel <erlab.analysis.fit.models.MultiPeakModel>`, which works like a composite model of multiple Gaussian or Lorentzian peaks.\n",
    "\n",
    "By supplying keyword arguments, you can specify the number of peaks, their shapes, whether to multiply with a Fermi-Dirac distribution, the shape of the background, and whether to convolve the result with experimental resolution.\n",
    "\n",
    "For a detailed explanation of all the arguments, see its {class}`documentation <erlab.analysis.fit.models.MultiPeakModel>`.\n",
    "\n",
    "The model can be constructed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = era.fit.models.MultiPeakModel(\n",
    "    npeaks=1, peak_shapes=[\"gaussian\"], fd=False, background=\"linear\", convolve=False\n",
    ")\n",
    "params = model.make_params(p0_center=5.0, p0_width=0.2, p0_height=3.0)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.fit(y, x=x, params=params, weights=1 / yerr)\n",
    "_ = result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = result.eval_components()\n",
    "plt.errorbar(x, y, yerr, fmt=\"o\", zorder=-1, alpha=0.3)\n",
    "plt.plot(x, result.eval(), label=\"Best fit\")\n",
    "plt.plot(x, comps[\"1Peak_p0\"], \"--\", label=\"Peak\")\n",
    "plt.plot(x, comps[\"1Peak_bkg\"], \"--\", label=\"Background\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us try fitting MDCs cut from simulated data with multiple Lorentzian peaks, convolved with a common instrumental resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from erlab.io.exampledata import generate_data\n",
    "\n",
    "data = generate_data(seed=1).T\n",
    "cut = data.qsel(ky=0.3)\n",
    "cut.qplot(colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdc = cut.qsel(eV=0.0)\n",
    "mdc.qplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the model and set the initial parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = era.fit.models.MultiPeakModel(\n",
    "    npeaks=2, peak_shapes=[\"lorentzian\"], fd=False, background=\"linear\", convolve=True\n",
    ")\n",
    "\n",
    "params = model.make_params(\n",
    "    p0_center=-0.5,\n",
    "    p1_center=0.5,\n",
    "    p0_width=0.03,\n",
    "    p1_width=0.03,\n",
    "    lin_bkg={\"value\": 0.0, \"vary\": False},\n",
    "    const_bkg=0.0,\n",
    "    resolution=0.03,\n",
    ")\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can fit the model to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.fit(mdc, x=mdc.kx, params=params)\n",
    "result.plot()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Fitting `xarray` objects\n",
    "\n",
    "ERLabPy provides accessors for xarray objects that allows you to fit data with lmfit models: {meth}`xarray.DataArray.modelfit <erlab.accessors.fit.ModelFitDataArrayAccessor.__call__>` or {meth}`xarray.Dataset.modelfit <erlab.accessors.fit.ModelFitDatasetAccessor.__call__>`, depending on whether you want to fit a single DataArray or multiple DataArrays in a Dataset. The accessor returns a {class}`xarray.Dataset` including the best-fit parameters and the fit statistics. The example below illustrates how to use the accessor to conduct the MDC fit demonstrated in the previous section.\n",
    "\n",
    ":::{hint}\n",
    "\n",
    "The syntax of the accessors are similar to the xarray methods {meth}`xarray.DataArray.curvefit` and {meth}`xarray.Dataset.curvefit`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ds = mdc.modelfit(\"kx\", model, params=params)\n",
    "result_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at data variables in the resulting Dataset.\n",
    "\n",
    "- `modelfit_results` contains the underlying {class}`lmfit.model.ModelResult` object from the fit.\n",
    "- `modelfit_coefficients` and `modelfit_stderr` contain the best-fit coefficients and their errors, respectively.\n",
    "- `modelfit_stats` contains the [goodness-of-fit statistics](https://lmfit.github.io/lmfit-py/fitting.html#goodness-of-fit-statistics).\n",
    "\n",
    "It may not be immediately obvious how this is useful, but the true power of the accessor comes from its ability to utilize xarray's powerful broadcasting capabilities described in the next section.\n",
    "\n",
    "### Fitting across multiple dimensions\n",
    "\n",
    ":::{note}\n",
    "\n",
    "There is a dedicated module for Fermi edge fitting and correction, described [here](fermi edge fitting). The following example is for illustrative purposes.\n",
    "\n",
    ":::\n",
    "\n",
    "Suppose you have to fit a single model to multiple data points across some dimension, or even multiple dimensions. The accessor can handle this with ease.\n",
    "\n",
    "Let's demonstrate this with a simulated cut that represents a curved Fermi edge at 100 K, with an energy broadening of 20 meV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from erlab.io.exampledata import generate_gold_edge\n",
    "\n",
    "gold = generate_gold_edge(temp=100, Eres=0.02, seed=1)\n",
    "gold.qplot(cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first select ± 0.2 eV around the Fermi level and fit the model across the energy\n",
    "axis for every EDC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_selected = gold.sel(eV=slice(-0.2, 0.2))\n",
    "result_ds = gold_selected.modelfit(\n",
    "    \"eV\",\n",
    "    era.fit.models.FermiEdgeModel(),\n",
    "    params={\"temp\": {\"value\": 100.0, \"vary\": False}},\n",
    "    guess=True,\n",
    ")\n",
    "result_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the data variables in the resulting Dataset now depend on the coordinate\n",
    "`alpha`. Let's plot the center of the edge as a function of angle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold.qplot(cmap=\"Greys\")\n",
    "plt.errorbar(\n",
    "    gold_selected.alpha,\n",
    "    result_ds.modelfit_coefficients.sel(param=\"center\"),\n",
    "    result_ds.modelfit_stderr.sel(param=\"center\"),\n",
    "    fmt=\".\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting is not limited to 1D models. The following example demonstrates a global fit to the cut with a multidimensional model. First, we normalize the data with the averaged intensity of each EDC and then fit the data to {class}`FermiEdge2dModel <erlab.analysis.fit.models.FermiEdge2dModel>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_norm = gold_selected / gold_selected.mean(\"eV\")\n",
    "result_ds = gold_norm.T.modelfit(\n",
    "    coords=[\"eV\", \"alpha\"],\n",
    "    model=era.fit.models.FermiEdge2dModel(),\n",
    "    params={\"temp\": {\"value\": 100.0, \"vary\": False}},\n",
    "    guess=True,\n",
    ")\n",
    "result_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the fit results and the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit = result_ds.modelfit_best_fit.transpose(*gold_norm.dims)\n",
    "\n",
    "fig, axs = eplt.plot_slices(\n",
    "    [gold_norm, best_fit, best_fit - gold_norm],\n",
    "    figsize=(4, 5),\n",
    "    cmap=[\"viridis\", \"viridis\", \"bwr\"],\n",
    "    norm=[plt.Normalize(), plt.Normalize(), eplt.CenteredPowerNorm(1.0, vcenter=0)],\n",
    "    colorbar=\"all\",\n",
    "    hide_colorbar_ticks=False,\n",
    "    colorbar_kw={\"width\": 7},\n",
    ")\n",
    "eplt.set_titles(axs, [\"Data\", \"FermiEdge2dModel\", \"Residuals\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providing multiple initial guesses\n",
    "\n",
    "You can also provide different initial guesses and bounds for different coordinates. To demonstrate, let's create some data containing multiple Gaussian peaks, each with a different center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define angle coordinates for 2D data\n",
    "alpha = np.linspace(-5.0, 5.0, 100)\n",
    "beta = np.linspace(-1.0, 1.0, 3)\n",
    "\n",
    "# Center of the peaks along beta\n",
    "center = np.array([-2.0, 0.0, 2.0])[:, np.newaxis]\n",
    "\n",
    "# Gaussian peak on a linear background\n",
    "y = -0.1 * alpha + 2 + 3 * np.exp(-((alpha - center) ** 2) / (2 * 1**2))\n",
    "\n",
    "# Add some noise with fixed seed for reproducibility\n",
    "rng = np.random.default_rng(5)\n",
    "yerr = np.full_like(y, 0.05)\n",
    "y = rng.normal(y, yerr)\n",
    "\n",
    "# Construct DataArray\n",
    "darr = xr.DataArray(y, dims=[\"beta\", \"alpha\"], coords={\"beta\": beta, \"alpha\": alpha})\n",
    "darr.qplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can provide different initial guesses for the peak positions along the `beta` dimension by passing a dictionary of DataArrays to the `params` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ds = darr.modelfit(\n",
    "    coords=\"alpha\",\n",
    "    model=GaussianModel() + LinearModel(),\n",
    "    params={\n",
    "        \"center\": xr.DataArray([-2, 0, 2], coords=[darr.beta]),\n",
    "        \"slope\": -0.1,\n",
    "    },\n",
    ")\n",
    "result_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's overlay the fitted peak positions on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ds.modelfit_data.qplot()\n",
    "result_center = result_ds.sel(param=\"center\")\n",
    "plt.plot(result_center.modelfit_coefficients, result_center.beta, \".-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be done with all parameter attributes that can be passed to {func}`lmfit.create_params` (e.g., `vary`, `min`, `max`, etc.). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ds = darr.modelfit(\n",
    "    coords=\"alpha\",\n",
    "    model=GaussianModel() + LinearModel(),\n",
    "    params={\n",
    "        \"center\": {\n",
    "            \"value\": xr.DataArray([-2, 0, 2], coords=[darr.beta]),\n",
    "            \"min\": -5.0,\n",
    "            \"max\": xr.DataArray([0, 2, 5], coords=[darr.beta]),\n",
    "        },\n",
    "        \"slope\": -0.1,\n",
    "    },\n",
    ")\n",
    "result_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The accessor works with any `lmfit` model, including background models from\n",
    "[lmfitxps](https://lmfitxps.readthedocs.io/). If you have\n",
    "[lmfitxps](https://lmfitxps.readthedocs.io/) installed, you can use the `ShirleyBG`\n",
    "model to iteratively fit a Shirley background to the data:\n",
    "```python\n",
    "from lmfitxps.models import ShirleyBG\n",
    "from lmfit.models import GaussianModel\n",
    "\n",
    "darr.modelfit(\"alpha\", GaussianModel() + ShirleyBG())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing fits\n",
    "\n",
    ":::{note}\n",
    "\n",
    "If you are viewing this documentation online, the plots will not be interactive. Run the code locally to try it out.\n",
    "\n",
    ":::\n",
    "\n",
    "If [hvplot](https://github.com/holoviz/hvplot) is installed, we can visualize the fit results interactively with the {class}`qshow <erlab.accessors.general.InteractiveDatasetAccessor>` accessor.\n",
    "\n",
    "To plot the data with the fit and fit components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ds.qshow(plot_components=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot each parameter as a function of the coordinate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ds.qshow.params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization\n",
    "\n",
    "The accessors are tightly integrated with `xarray`, so passing a dask array will parallelize the fitting process. See [Parallel Computing with Dask](https://docs.xarray.dev/en/stable/user-guide/dask.html) for more information.\n",
    "\n",
    "For non-dask objects, you can achieve [joblib](https://joblib.readthedocs.io/en/stable/)-based parallelization:\n",
    "\n",
    "- For non-dask Datasets, basic parallelization across multiple data variables can be achieved with the `parallel` argument to {meth}`xarray.Dataset.modelfit <erlab.accessors.fit.ModelFitDatasetAccessor.__call__>`.\n",
    "\n",
    "- For parallelizing fits on a single DataArray along a dimension with many points, the {meth}`xarray.DataArray.parallel_fit <erlab.accessors.fit.ParallelFitDataArrayAccessor>` accessor can be used. This method is similar to {meth}`xarray.DataArray.modelfit <erlab.accessors.fit.ModelFitDataArrayAccessor.__call__>`, but requires the name of the dimension to parallelize over instead of the coordinates to fit along. For example, to parallelize the fit in the previous example, you can use the following code:\n",
    "\n",
    "    ```python\n",
    "\n",
    "    gold_selected.parallel_fit(\n",
    "        dim=\"alpha\",\n",
    "        model=FermiEdgeModel(),\n",
    "        params={\"temp\": {\"value\": 100.0, \"vary\": False}},\n",
    "        guess=True,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    :::{note}\n",
    "  \n",
    "    - Note that the initial run will take a long time due to the overhead of creating parallel workers. Subsequent calls will run faster, since joblib's default backend will try to reuse the workers.\n",
    "      \n",
    "    - The accessor has some intrinsic overhead due to post-processing. If you need the best performance, handle the parallelization yourself with joblib and {meth}`lmfit.Model.fit <lmfit.model.Model.fit>`.\n",
    "\n",
    "    :::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading fits\n",
    "\n",
    "Since the resulting dataset contains no Python objects, it can be saved and loaded reliably as a netCDF file using {meth}`xarray.Dataset.to_netcdf` and {func}`xarray.open_dataset`, respectively. If you wish to obtain the {class}`lmfit.model.ModelResult` objects from the fit, you can use the `output_result` argument.\n",
    "\n",
    ":::{warning}\n",
    "\n",
    "    Saving full model results that includes the model functions can be difficult. Instead of saving the fit results, it is recommended to save the code that can reproduce the fit. See [the relevant lmfit documentation](https://lmfit.github.io/lmfit-py/model.html#saving-and-loading-modelresults) for more information.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `iminuit`\n",
    "\n",
    ":::{note}\n",
    "\n",
    "This part requires the optional [iminuit](https://github.com/scikit-hep/iminuit) dependency.\n",
    "\n",
    ":::\n",
    "\n",
    "[iminuit](https://github.com/scikit-hep/iminuit) is a powerful Python interface to the [Minuit C++ library](https://root.cern.ch/doc/master/Minuit2Page.html) developed at CERN. To learn more, see the [iminuit documentation](http://scikit-hep.org/iminuit/).\n",
    "\n",
    "ERLabPy provides a thin wrapper around {class}`iminuit.Minuit` that allows you to use lmfit models with iminuit. The example below conducts the same fit as the previous one, but using iminuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = era.fit.models.MultiPeakModel(\n",
    "    npeaks=2, peak_shapes=[\"lorentzian\"], fd=False, convolve=True\n",
    ")\n",
    "\n",
    "m = era.fit.minuit.Minuit.from_lmfit(\n",
    "    model,\n",
    "    mdc,\n",
    "    mdc.kx,\n",
    "    p0_center=-0.5,\n",
    "    p1_center=0.5,\n",
    "    p0_width=0.03,\n",
    "    p1_width=0.03,\n",
    "    p0_height=1000,\n",
    "    p1_height=1000,\n",
    "    lin_bkg={\"value\": 0.0, \"vary\": False},\n",
    "    const_bkg=0.0,\n",
    "    resolution=0.03,\n",
    ")\n",
    "\n",
    "m.migrad()\n",
    "m.minos()\n",
    "m.hesse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the [interactive fitting interface](https://scikit-hep.org/iminuit/notebooks/interactive.html) provided by iminuit.\n",
    "\n",
    ":::{note}\n",
    "\n",
    "- Requires [ipywidgets](https://github.com/jupyter-widgets/ipywidgets) to be installed.\n",
    "    \n",
    "- If you are viewing this documentation online, changing the sliders won’t change the plot. run the code locally to try it out.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "m.interactive()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
