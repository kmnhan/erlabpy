{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading & writing data\n",
    "======================"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "In ERLabPy, most data are represented as :class:`xarray.Dataset` objects or\n",
    ":class:`xarray.DataArray` objects.\n",
    "\n",
    ":class:`xarray.DataArray` are similar to waves in Igor Pro, but are much more flexible.\n",
    "Opposed to the maximum of 4 dimensions in Igor, :class:`xarray.DataArray` can have as\n",
    "many dimensions as you want (up to 64). Another advantage is that the coordinates of the\n",
    "dimensions do not have to be evenly spaced. In fact, they are not limited to numbers but\n",
    "can be any type of data, such as date and time representations.\n",
    "\n",
    ":class:`xarray.Dataset` is a collection of :class:`xarray.DataArray` objects. It is used to\n",
    "store multiple data arrays that are related to each other, such as a set of measurements.\n",
    "\n",
    ".. note::\n",
    "\n",
    "    If you are not familiar with :mod:`xarray`, it is recommended to read the `xarray\n",
    "    tutorial <https://tutorial.xarray.dev/>`_ and the `xarray user guide\n",
    "    <https://docs.xarray.dev/en/stable/user-guide/index.html>`_ first.\n",
    "\n",
    "This guide will introduce you to reading and writing data from and to various file\n",
    "formats, and how to implement a custom plugin for a experimental setup.\n",
    "\n",
    "Skip to the `corresponding section <#Loading-ARPES-data>`_ for guides on loading ARPES\n",
    "data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data\n",
    "------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ":mod:`xarray` provides native support for reading and writing NetCDF and HDF5 files into\n",
    ":mod:`xarray` objects. See the :mod:`xarray` documentation on `I/O operations\n",
    "<https://docs.xarray.dev/en/stable/user-guide/io.html>`_.\n",
    "\n",
    "Here, we will focus on working with Igor Pro and xarray objects."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "From Igor Pro\n",
    "~~~~~~~~~~~~~\n",
    "\n",
    "Installing ERLabPy automatically registers a backend for xarray that allows reading\n",
    "``.pxt``, ``.pxp``, and ``.ibw`` files. This means that you can load these files\n",
    "directly into xarray using :func:`xarray.open_dataset` or :func:`xarray.open_dataarray`\n",
    "as if they were NetCDF files.\n",
    "\n",
    "In most cases, xarray will automatically detect the file format. For example, to load a\n",
    "``.ibw`` file into a :class:`xarray.DataArray`, use the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import xarray as xr\n",
    "\n",
    "data = xr.open_dataarray(\"path/to/file.ibw\")\n",
    "```\n",
    "\n",
    "Along with the Igor Pro file formats, the backend also supports loading HDF5 files\n",
    "exported from Igor Pro. For such files, the engine must be specified explicitly with\n",
    "`engine=\"erlab-igor\"`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ".. warning::\n",
    "\n",
    "   Loading waves from complex ``.pxp`` files may fail or produce unexpected results. It\n",
    "   is recommended to export the waves to a ``.ibw`` file to load them in ERLabPy. If you\n",
    "   encounter any problems, please let us know by opening an issue.\n",
    "\n",
    "Convenience functions that can load Igor Pro files directly can be found in\n",
    ":mod:`erlab.io.igor`. However, it is recommended to use the xarray functions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "From arbitrary formats\n",
    "~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "There are many python libraries that can read and write data in various formats. Here,\n",
    "some common file formats and how to read them are listed:\n",
    "\n",
    "* Spreadsheet data can be read using :func:`pandas.read_csv` and\n",
    "  :func:`pandas.read_excel`.\n",
    "  \n",
    "  The resulting DataFrame can be converted to an xarray object using\n",
    "  :meth:`pandas.DataFrame.to_xarray` or :meth:`xarray.Dataset.from_dataframe`.\n",
    "\n",
    "* When reading HDF5 files with arbitrary groups and metadata, you must first explore the \n",
    "  group structure using `h5netcdf <https://h5netcdf.org/>`_ or ``open_datatree``.\n",
    "\n",
    "  .. note::\n",
    "  \n",
    "     ``open_datatree`` can be imported from ``xarray.backends.api``. In the near future,\n",
    "     it will be documented and made available in the public API.\n",
    "  \n",
    "  Loading a specific HDF5 group into an xarray object can be done using\n",
    "  :func:`xarray.open_dataset` or :func:`xarray.open_mfdataset` by supplying the\n",
    "  ``group`` argument.\n",
    "\n",
    "* FITS files can be read with `astropy\n",
    "  <https://docs.astropy.org/en/stable/io/fits/index.html>`_. In the near future, ERLabPy\n",
    "  will provide a loader for FITS files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing data\n",
    "------------\n",
    "Since the state and variables of a Python interpreter are not saved, it is important to\n",
    "save your data in a format that can be easily read and written.\n",
    "\n",
    "While it is possible to save and load entire Python interpreter sessions using `pickle`\n",
    "or the more versatile [dill](https://github.com/uqfoundation/dill), it is out of the\n",
    "scope of this guide. Instead, we recommend saving your data in a format that is easy to\n",
    "read and write, such as HDF5 or NetCDF. These formats are supported by many programming\n",
    "languages and are optimized for fast read and write operations.\n",
    "\n",
    "To save and load `xarray` objects, see the `xarray` documentation on [I/O operations](https://docs.xarray.dev/en/stable/user-guide/io.html)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "To Igor Pro\n",
    "~~~~~~~~~~~\n",
    "\n",
    "As an experimental feature, :func:`save_as_hdf5 <erlab.io.save_as_hdf5>` can save\n",
    "certain :class:`xarray.DataArray`\\ s in a format that is compatible with the Igor Pro\n",
    "HDF5 loader. An `accompanying Igor procedure\n",
    "<https://github.com/kmnhan/erlabpy/blob/main/PythonInterface.ipf>`_ is available in the\n",
    "repository. If loading in Igor Pro fails, try saving again with all attributes removed.\n",
    "\n",
    "Alternatively, `igorwriter <https://github.com/t-onoz/igorwriter>`_ can be used to write\n",
    "numpy arrays to ``.ibw`` and ``.itx`` files directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading ARPES data\n",
    "------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ".. warning::\n",
    "\n",
    "   ERLabPy is still in development and the API may change. Some major changes regarding\n",
    "   data loading and handling are planned:\n",
    "\n",
    "   - The `xarray datatree structure <https://github.com/pydata/xarray/issues/8572>`_\n",
    "     will enable much more intuitive and powerful data handling. Once the feature gets\n",
    "     incorporated into xarray, ERLabPy will be updated to use it.\n",
    "\n",
    "   - A universal translation layer between true data header attributes and\n",
    "     human-readable representations will be implemented. This will allow for more\n",
    "     consistent and user-friendly data handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERLabPy's data loading framework consists of various plugins, or *loaders*, each\n",
    "designed to load data from a different beamline or laboratory. Each loader is a class\n",
    "that has a `load` method which takes a file path or sequence number and returns data.\n",
    "\n",
    "Let's see the list of loaders available by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "notebookRunGroups": {
     "groupValue": ""
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import erlab.io\n",
    "\n",
    "erlab.io.loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = [\"svg\", \"pdf\"]\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 96\n",
    "plt.rcParams[\"image.cmap\"] = \"viridis\"\n",
    "\n",
    "xr.set_options(display_expand_data=False)\n",
    "nb_execution_mode = \"cache\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "You can access each loader by its name or alias, both as an attribute or as an item. For\n",
    "example, to access the loader for the ALS beamline 4.0.3, you can use any of the\n",
    "following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "erlab.io.loaders[\"merlin\"]\n",
    "erlab.io.loaders[\"bl403\"]\n",
    "erlab.io.loaders.merlin\n",
    "erlab.io.loaders.bl403"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Data loading is done by calling the :meth:`load <erlab.io.dataloader.LoaderBase.load>`\n",
    "method of the loader. It requires an ``identifier`` parameter, which can be a path to a\n",
    "file or a sequence number. It also accepts a ``data_dir`` parameter, which specifies the\n",
    "directory where the data is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If `identifier` is a sequence number, `data_dir` must be provided.\n",
    "\n",
    "- If `identifier` is a string and `data_dir` is provided, the path is constructed by\n",
    "  joining `data_dir` and `identifier`.\n",
    "\n",
    "- If `identifier` is a string and `data_dir` is not provided, `identifier` should be a\n",
    "  valid path to a file.\n",
    "\n",
    "Suppose we have data from the ALS beamline 4.0.3 stored as `/path/to/data/f_001.pxt`,\n",
    "`/path/to/data/f_002.pxt`, etc. To load `f_001.pxt`, all three of the following are\n",
    "valid:\n",
    "\n",
    "```python\n",
    "loader = erlab.io.loaders[\"merlin\"]\n",
    "\n",
    "loader.load(\"/path/to/data/f_001.pxt\")\n",
    "loader.load(\"f_001.pxt\", data_dir=\"/path/to/data\")\n",
    "loader.load(1, data_dir=\"/path/to/data\")\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Setting the default loader and data directory\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "In practice, a loader and a single directory will be used repeatedly in a session to\n",
    "load different data from the same experiment.\n",
    "\n",
    "Instead of explicitly specifying the loader and directory each time, a default loader\n",
    "and data directory can be set with :func:`erlab.io.set_loader` and\n",
    ":func:`erlab.io.set_data_dir`. All subsequent calls to the shortcut function\n",
    ":func:`erlab.io.load` will use the specified loader and data directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "erlab.io.set_loader(\"merlin\")\n",
    "erlab.io.set_data_dir(\"/path/to/data\")\n",
    "data_1 = erlab.io.load(1)\n",
    "data_2 = erlab.io.load(2)\n",
    "```\n",
    "\n",
    "The loader and data directory can also be controlled with a context manager:\n",
    "\n",
    "```python\n",
    "with erlab.io.loader_context(\"merlin\", data_dir=\"/path/to/data\"):\n",
    "    data_1 = erlab.io.load(1)\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Data across multiple files\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For setups like the ALS beamline 4.0.3, some scans are stored over multiple files like\n",
    "`f_003_S001.pxt`, `f_003_S002.pxt`, and so on. In this case, the loader will\n",
    "automatically concatenate all files in the same scan. For example, *all of the\n",
    "following* will return the same concatenated data:\n",
    "\n",
    "```python\n",
    "erlab.io.load(3)\n",
    "erlab.io.load(\"f_003_S001.pxt\")\n",
    "erlab.io.load(\"f_003_S002.pxt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "If you want to cherry-pick a single file, you can pass ``single=True`` to :meth:`load\n",
    "<erlab.io.dataloader.LoaderBase.load>`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "erlab.io.load(\"f_003_S001.pxt\", single=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Handling multiple data directories\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "   \n",
    "If you call :func:`erlab.io.set_loader` or :func:`erlab.io.set_data_dir` multiple times,\n",
    "the last call will override the previous ones. While this is useful for changing the\n",
    "loader or data directory, it makes data loading *dependent on execution order*. This may\n",
    "lead to unexpected behavior.\n",
    "\n",
    "If you plan to use multiple loaders or data directories in the same session, it is\n",
    "recommended to use the context manager. If you have to load data from multiple\n",
    "directories multiple times, it may be convenient to define functions that set the loader\n",
    "and data directory and call :func:`erlab.io.load` with the appropriate arguments. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def load1(identifier):\n",
    "    with erlab.io.loader_context(\"merlin\", data_dir=\"/path/to/data1\"):\n",
    "        return erlab.io.load(identifier)\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Summarizing data\n",
    "~~~~~~~~~~~~~~~~\n",
    "\n",
    "Some loaders have :meth:`generate_summary\n",
    "<erlab.io.dataloader.LoaderBase.generate_summary>` implemented, which generates a\n",
    ":class:`pandas.DataFrame` containing an overview of the data in a given directory. The\n",
    "generated summary can be viewed as a table with the :meth:`summarize\n",
    "<erlab.io.dataloader.LoaderBase.summarize>` method. If ``ipywidgets`` is installed, an\n",
    "interactive widget is also displayed. This is useful for quickly skimming through the\n",
    "data.\n",
    "\n",
    "Just like :meth:`load <erlab.io.dataloader.LoaderBase.load>`, :meth:`summarize\n",
    "<erlab.io.dataloader.LoaderBase.summarize>` can also be accessed with the shortcut\n",
    "function :func:`erlab.io.summarize`. For example, to display a summary of the data\n",
    "available in the directory ``/path/to/data`` using the ``'merlin'`` loader:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "erlab.io.set_loader(\"merlin\")\n",
    "erlab.io.set_data_dir(\"/path/to/data\")\n",
    "erlab.io.summarize()\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "To see what the generated summary looks like, see the :ref:`example below <summary\n",
    "example>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a data loader plugin \n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "It is easy to add new loaders to the framework. Any subclass of :class:`LoaderBase\n",
    "<erlab.io.dataloader.LoaderBase>` is automatically registered as a loader! The class\n",
    "must have a valid :attr:`name <erlab.io.dataloader.LoaderBase.name>` attribute which is\n",
    "used to access the loader.\n",
    "\n",
    "If the :attr:`name <erlab.io.dataloader.LoaderBase.name>` attribute is prefixed with an\n",
    "underscore, the registration is skipped. This is useful for base classes that are not\n",
    "meant to be used directly.\n",
    "\n",
    "Data loading flow\n",
    "~~~~~~~~~~~~~~~~~\n",
    "\n",
    "ARPES data from a single experiment are usually stored in one folder, with files that\n",
    "look like ``file_0001.h5``, ``file_0002.h5``, etc. If the naming scheme does not deviate\n",
    "from this pattern, only two methods need to be implemented: :meth:`identify\n",
    "<erlab.io.dataloader.LoaderBase.identify>` and :meth:`load_single\n",
    "<erlab.io.dataloader.LoaderBase.load_single>`. The following flowchart shows the process\n",
    "of loading data from a single scan, given the path to the directory (``data_dir``) and\n",
    "the sequence number or file name (``identifier``):\n",
    "\n",
    ".. image:: ../images/flowchart_single.pdf\n",
    "   :align: center\n",
    "   \n",
    "Here, :meth:`identify <erlab.io.dataloader.LoaderBase.identify>` is given an integer\n",
    "sequence number(``identifier``) and the path to the data folder(``data_dir``), and\n",
    "returns the full path to the corresponding data file. \n",
    "\n",
    "The method :meth:`load_single <erlab.io.dataloader.LoaderBase.load_single>` is given a\n",
    "full path to a single data file and must return the data as an :class:`xarray.DataArray`\n",
    "or a :class:`xarray.Dataset`. If the data cannot be combined into a single object, the\n",
    "method can also return a list of :class:`xarray.DataArray` objects.\n",
    "\n",
    "If only all data formats were as simple as this! Unfortunately, there are some setups\n",
    "where data for a single scan is saved over multiple files. In this case, the files will\n",
    "look like ``file_0001_0001.h5``, ``file_0001_0002.h5``, etc. For these kinds of setups,\n",
    "an additional method :meth:`infer_index <erlab.io.dataloader.LoaderBase.infer_index>`\n",
    "must be implemented. The following flowchart shows the process of loading data from\n",
    "multiple files:\n",
    "\n",
    ".. image:: ../images/flowchart_multiple.pdf\n",
    "   :align: center\n",
    "\n",
    "In this case, the method :meth:`identify <erlab.io.dataloader.LoaderBase.identify>`\n",
    "should resolve *all* files that belong to the given sequence number, and return a list\n",
    "of file paths along with a dictionary of corresponding coordinates.\n",
    "\n",
    "The method :meth:`infer_index <erlab.io.dataloader.LoaderBase.infer_index>` is given a\n",
    "bare file name (without the extension and path) and must return the sequence number of\n",
    "the scan. For example, given the file name ``file_0003_0123``, the method should return\n",
    "``3``.\n",
    "\n",
    "Conventions\n",
    "~~~~~~~~~~~\n",
    "\n",
    "There are some rules that loaded ARPES data must follow to ensure that analysis\n",
    "procedures such as momentum conversion and fitting works seamlessly:\n",
    "\n",
    "- The experimental geometry should be stored in the ``'configuration'`` attribute as an\n",
    "  integer. See :ref:`Nomenclature <nomenclature>` and :class:`AxesConfiguration\n",
    "  <erlab.analysis.kspace.AxesConfiguration>` for more information.\n",
    "- All standard angle coordinates must follow the naming conventions in\n",
    "  :ref:`Nomenclature <nomenclature>`.\n",
    "- The sample temperature, if available, should be stored in the ``'temp_sample'``\n",
    "  attribute.\n",
    "- The sample work function, if available, should be stored in the\n",
    "  ``'sample_workfunction'`` attribute.\n",
    "- Energies should be given in electronvolts.\n",
    "- Angles should be given in degrees.\n",
    "- Temperatures should be given in Kelvins.\n",
    "\n",
    "All loaders by default does a basic check for a subset of these rules using\n",
    ":meth:`validate <erlab.io.dataloader.LoaderBase.validate>` and will raise a warning if\n",
    "some are missing. This behavior can be controlled with loader class attributes\n",
    ":attr:`skip_validate <erlab.io.dataloader.LoaderBase.skip_validate>` and\n",
    ":attr:`strict_validation <erlab.io.dataloader.LoaderBase.strict_validation>`.\n",
    "\n",
    "A minimal example\n",
    "~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Consider a setup that saves data into a ``.csv`` file named ``data_0001.csv`` and so on.\n",
    "A bare minimum implementation of a loader for the setup will look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from erlab.io.dataloader import LoaderBase\n",
    "\n",
    "\n",
    "class MyLoader(LoaderBase):\n",
    "    name = \"my_loader\"\n",
    "    aliases = None\n",
    "    name_map = {}\n",
    "    coordinate_attrs = {}\n",
    "    additional_attrs = {\"information\": \"any metadata you want to load with the data\"}\n",
    "    skip_validate = False\n",
    "    always_single = True\n",
    "\n",
    "    def identify(self, num, data_dir):\n",
    "        file = os.path.join(data_dir, f\"data_{str(num).zfill(4)}.csv\")\n",
    "        return [file], {}\n",
    "\n",
    "    def load_single(self, file_path):\n",
    "        return pd.read_csv(file_path).to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "erlab.io.loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "erlab.io.loaders[\"my_loader\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "We can see that the loader has been registered.\n",
    "\n",
    "A complex example\n",
    "~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Next, let's try to write a more realistic loader for a hypothetical setup that saves\n",
    "data as HDF5 files with the following naming scheme: ``data_001.h5``, ``data_002.h5``,\n",
    "and so on, with multiple scans named like ``data_001_S001.h5``, ``data_001_S002.h5``,\n",
    "etc. with the scan axis information stored in a separate file named\n",
    "``data_001_axis.csv``.\n",
    "\n",
    "Let us first generate a data directory and place some synthetic data in it. Before\n",
    "saving, we rename and set some attributes that resemble real ARPES data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import erlab.io\n",
    "from erlab.io.exampledata import generate_data_angles\n",
    "\n",
    "\n",
    "def make_data(beta=5.0, temp=20.0, hv=50.0, bandshift=0.0):\n",
    "    data = generate_data_angles(\n",
    "        shape=(250, 1, 300),\n",
    "        angrange={\"alpha\": (-15, 15), \"beta\": (beta, beta)},\n",
    "        hv=hv,\n",
    "        configuration=1,\n",
    "        temp=temp,\n",
    "        bandshift=bandshift,\n",
    "        assign_attributes=False,\n",
    "        seed=1,\n",
    "    ).T\n",
    "\n",
    "    # Rename coordinates. The loader must rename them back to the original names.\n",
    "    data = data.rename(\n",
    "        {\n",
    "            \"alpha\": \"ThetaX\",\n",
    "            \"beta\": \"Polar\",\n",
    "            \"eV\": \"BindingEnergy\",\n",
    "            \"hv\": \"PhotonEnergy\",\n",
    "            \"xi\": \"Tilt\",\n",
    "            \"delta\": \"Azimuth\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Assign some attributes that real data would have\n",
    "    data = data.assign_attrs(\n",
    "        {\n",
    "            \"LensMode\": \"Angular30\",  # Lens mode of the analyzer\n",
    "            \"SpectrumType\": \"Fixed\",  # Acquisition mode of the analyzer\n",
    "            \"PassEnergy\": 10,  # Pass energy of the analyzer\n",
    "            \"UndPol\": 0,  # Undulator polarization\n",
    "            \"DateTime\": datetime.datetime.now().isoformat(),  # Acquisition time\n",
    "            \"TB\": temp,\n",
    "            \"X\": 0.0,\n",
    "            \"Y\": 0.0,\n",
    "            \"Z\": 0.0,\n",
    "        }\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "# Create a temporary directory\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "# Define coordinates for the scan\n",
    "beta_coords = np.linspace(2, 7, 10)\n",
    "\n",
    "# Generate and save cuts with different beta values\n",
    "for i, beta in enumerate(beta_coords):\n",
    "    data = make_data(beta=beta, temp=20.0, hv=50.0)\n",
    "    filename = f\"{tmp_dir.name}/data_001_S{str(i+1).zfill(3)}.h5\"\n",
    "    data.to_netcdf(filename, engine=\"h5netcdf\")\n",
    "\n",
    "# Write scan coordinates to a csv file\n",
    "with open(f\"{tmp_dir.name}/data_001_axis.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Index\", \"Polar\"])\n",
    "\n",
    "    for i, beta in enumerate(beta_coords):\n",
    "        writer.writerow([i + 1, beta])\n",
    "\n",
    "# Generate some cuts with different band shifts\n",
    "for i in range(4):\n",
    "    data = make_data(beta=5.0, temp=20.0, hv=50.0, bandshift=-i * 0.05)\n",
    "    filename = f\"{tmp_dir.name}/data_{str(i+2).zfill(3)}.h5\"\n",
    "    data.to_netcdf(filename, engine=\"h5netcdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have generated a folder that resembles typical data from an ARPES experiment. Let's list the contents of the folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(os.listdir(tmp_dir.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each HDF5 file represents a single ARPES cut. `data_001_S001.h5` to `data_001_S010.h5`\n",
    "represents an ARPES map with 10 cuts, with the scan axis recorded in\n",
    "`data_001_axis.csv`. Let's check what the raw data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.load_dataarray(f\"{tmp_dir.name}/data_002.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been properly loaded, but the coordinates and attrubutes have names that\n",
    "are specific to the beamline.\n",
    "\n",
    "Our loader should do three things: rename the coordinates and attributes to standard\n",
    "names, add metadata to the dataset, and combine related cuts into a single DataArray\n",
    "that contains the ARPES mapping."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ".. note ::\n",
    "\n",
    "    Here, we easily loaded the data into an xarray object directly, but that is not the\n",
    "    case for most experimental setups. Properly loading raw data into an xarray object\n",
    "    is a complex process that requires knowledge of the data format and the experimental\n",
    "    setup, and this is what must be implemented in the :meth:`load_single\n",
    "    <erlab.io.dataloader.LoaderBase.load_single>`.\n",
    "\n",
    "    ERLabPy provides convenient functions to ease this process. See `implementations of\n",
    "    existing data loaders\n",
    "    <https://github.com/kmnhan/erlabpy/tree/main/src/erlab/io/plugins>`_ for examples.\n",
    "\n",
    "Now that we have the data, let's implement the loader. The biggest difference from the\n",
    "previous example is that we need to handle multiple files for a single scan in\n",
    ":meth:`identify <erlab.io.dataloader.LoaderBase.identify>`. Also, we have to implement\n",
    ":meth:`infer_index <erlab.io.dataloader.LoaderBase.infer_index>` to extract the scan\n",
    "number from the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "from erlab.io.dataloader import LoaderBase\n",
    "\n",
    "\n",
    "class ExampleLoader(LoaderBase):\n",
    "    name = \"example\"\n",
    "\n",
    "    aliases = [\"Ex\"]\n",
    "\n",
    "    name_map = {\n",
    "        \"eV\": \"BindingEnergy\",\n",
    "        \"alpha\": \"ThetaX\",\n",
    "        \"beta\": [\n",
    "            \"Polar\",\n",
    "            \"Polar Compens\",\n",
    "        ],  # Can have multiple names assigned to the same name\n",
    "        # If both are present in the data, a ValueError will be raised\n",
    "        \"delta\": \"Azimuth\",\n",
    "        \"xi\": \"Tilt\",\n",
    "        \"x\": \"X\",\n",
    "        \"y\": \"Y\",\n",
    "        \"z\": \"Z\",\n",
    "        \"hv\": \"PhotonEnergy\",\n",
    "        \"polarization\": \"UndPol\",\n",
    "        \"temp_sample\": \"TB\",\n",
    "    }\n",
    "\n",
    "    coordinate_attrs: tuple[str, ...] = (\n",
    "        \"beta\",\n",
    "        \"delta\",\n",
    "        \"xi\",\n",
    "        \"hv\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        \"z\",\n",
    "        \"polarization\",\n",
    "        \"photon_flux\",\n",
    "    )\n",
    "    # Attributes to be used as coordinates. Place all attributes that we don't want to\n",
    "    # lose when merging multiple file scans here.\n",
    "\n",
    "    additional_attrs = {\n",
    "        \"configuration\": 1,  # Experimental geometry. Required for momentum conversion\n",
    "        \"sample_workfunction\": 4.3,\n",
    "    }\n",
    "    # Any additional metadata you want to add to the data. Note that attributes defined\n",
    "    # here will not be transformed into coordinates. If you wish to promote some fixed\n",
    "    # attributes to coordinates, add them to additional_coords.\n",
    "\n",
    "    additional_coords = {}\n",
    "    # Additional non-dimension coordinates to be added to the data, for instance the\n",
    "    # photon energy for lab-based ARPES.\n",
    "\n",
    "    skip_validate = False\n",
    "\n",
    "    always_single = False\n",
    "\n",
    "    def identify(self, num, data_dir):\n",
    "        coord_dict = {}\n",
    "\n",
    "        # Look for scans with data_###_S###.h5, and sort them\n",
    "        files = glob.glob(f\"data_{str(num).zfill(3)}_S*.h5\", root_dir=data_dir)\n",
    "        files.sort()\n",
    "\n",
    "        if len(files) == 0:\n",
    "            # If no files found, look for data_###.h5\n",
    "            files = glob.glob(f\"data_{str(num).zfill(3)}.h5\", root_dir=data_dir)\n",
    "\n",
    "        else:\n",
    "            # If files found, extract coordinate values from the filenames\n",
    "            axis_file = f\"{data_dir}/data_{str(num).zfill(3)}_axis.csv\"\n",
    "            with open(axis_file) as f:\n",
    "                header = f.readline().strip().split(\",\")\n",
    "\n",
    "            # Load the coordinates from the csv file\n",
    "            coord_arr = np.loadtxt(axis_file, delimiter=\",\", skiprows=1)\n",
    "\n",
    "            # Each header entry will contain a dimension name\n",
    "            for i, hdr in enumerate(header[1:]):\n",
    "                coord_dict[hdr] = coord_arr[: len(files), i + 1].astype(np.float64)\n",
    "\n",
    "        if len(files) == 0:\n",
    "            # If no files found up to this point, return None\n",
    "            return None\n",
    "\n",
    "        # Files must be full paths\n",
    "        files = [os.path.join(data_dir, f) for f in files]\n",
    "\n",
    "        return files, coord_dict\n",
    "\n",
    "    def load_single(self, file_path):\n",
    "        return xr.open_dataarray(file_path, engine=\"h5netcdf\")\n",
    "\n",
    "    def infer_index(self, name):\n",
    "        # Get the scan number from file name\n",
    "        try:\n",
    "            scan_num: str = re.match(r\".*?(\\d{3})(?:_S\\d{3})?\", name).group(1)\n",
    "        except (AttributeError, IndexError):\n",
    "            return None, None\n",
    "\n",
    "        if scan_num.isdigit():\n",
    "            # The second return value, a dictionary, is reserved for more complex\n",
    "            # setups. See tips below for a brief explanation.\n",
    "            return int(scan_num), {}\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erlab.io.loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `example` loader has been registered. Let's test the loader by\n",
    "loading and plotting some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erlab.io.set_loader(\"example\")\n",
    "erlab.io.set_data_dir(tmp_dir.name)\n",
    "erlab.io.load(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erlab.io.load(5).qplot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Brilliant! We now have a working loader for our hypothetical setup. However, we can't\n",
    "use :func:`erlab.io.summarize` with our loader since we haven't implemented\n",
    ":meth:`generate_summary <erlab.io.dataloader.LoaderBase.generate_summary>`.\n",
    "\n",
    "This method should return a :class:`pandas.DataFrame` with the index containing file\n",
    "names. The only requirement for the DataFrame is that it should include a column named\n",
    "``'Path'`` that contains the paths to the data files. Other than that, the DataFrame can\n",
    "contain any metadata you wish to display in the summary. Let's implement it in a\n",
    "subclass of the ``example`` loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleLoaderComplete(ExampleLoader):\n",
    "    name = \"example_complete\"\n",
    "    aliases = [\"ExC\"]\n",
    "\n",
    "    def generate_summary(self, data_dir):\n",
    "        # Get all valid data files in directory\n",
    "        files = {}\n",
    "        for path in erlab.io.utils.get_files(data_dir, extensions=[\".h5\"]):\n",
    "            # Base name\n",
    "            data_name = os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "            # If multiple scans, strip the _S### part\n",
    "            name_match = re.match(r\"(.*?_\\d{3})_(?:_S\\d{3})?\", data_name)\n",
    "            if name_match is not None:\n",
    "                data_name = name_match.group(1)\n",
    "\n",
    "            files[data_name] = path\n",
    "\n",
    "        # Map dataframe column names to data attributes\n",
    "        attrs_mapping = {\n",
    "            \"Lens Mode\": \"LensMode\",\n",
    "            \"Scan Type\": \"SpectrumType\",\n",
    "            \"T(K)\": \"temp_sample\",\n",
    "            \"Pass E\": \"PassEnergy\",\n",
    "            \"Polarization\": \"polarization\",\n",
    "            \"hv\": \"hv\",\n",
    "            \"x\": \"x\",\n",
    "            \"y\": \"y\",\n",
    "            \"z\": \"z\",\n",
    "            \"polar\": \"beta\",\n",
    "            \"tilt\": \"xi\",\n",
    "            \"azi\": \"delta\",\n",
    "        }\n",
    "        column_names = [\"File Name\", \"Path\", \"Time\", \"Type\", *attrs_mapping.keys()]\n",
    "\n",
    "        data_info = []\n",
    "\n",
    "        processed_indices = set()\n",
    "        for name, path in files.items():\n",
    "            # Skip already processed multi-file scans\n",
    "            index, _ = self.infer_index(name)\n",
    "            if index in processed_indices:\n",
    "                continue\n",
    "            if index is not None:\n",
    "                processed_indices.add(index)\n",
    "\n",
    "            # Load data\n",
    "            data = self.load(path)\n",
    "\n",
    "            # Determine type of scan\n",
    "            data_type = \"core\"\n",
    "            if \"alpha\" in data.dims:\n",
    "                data_type = \"cut\"\n",
    "            if \"beta\" in data.dims:\n",
    "                data_type = \"map\"\n",
    "            if \"hv\" in data.dims:\n",
    "                data_type = \"hvdep\"\n",
    "\n",
    "            data_info.append(\n",
    "                [\n",
    "                    name,\n",
    "                    path,\n",
    "                    datetime.datetime.fromisoformat(data.attrs[\"DateTime\"]),\n",
    "                    data_type,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            for k, v in attrs_mapping.items():\n",
    "                # Try to get the attribute from the data, then from the coordinates\n",
    "                try:\n",
    "                    val = data.attrs[v]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        val = data.coords[v].values\n",
    "                        if val.size == 1:\n",
    "                            val = val.item()\n",
    "                    except KeyError:\n",
    "                        val = \"\"\n",
    "\n",
    "                # Convert polarization values to human readable form\n",
    "                if k == \"Polarization\":\n",
    "                    if np.iterable(val):\n",
    "                        val = np.asarray(val).astype(int)\n",
    "                    else:\n",
    "                        val = [round(val)]\n",
    "                    val = [{0: \"LH\", 2: \"LV\", -1: \"RC\", 1: \"LC\"}.get(v, v) for v in val]\n",
    "                    if len(val) == 1:\n",
    "                        val = val[0]\n",
    "\n",
    "                data_info[-1].append(val)\n",
    "\n",
    "            del data\n",
    "\n",
    "        # Sort by time and set index\n",
    "        return (\n",
    "            pd.DataFrame(data_info, columns=column_names)\n",
    "            .sort_values(\"Time\")\n",
    "            .set_index(\"File Name\")\n",
    "        )\n",
    "\n",
    "\n",
    "erlab.io.loaders"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ".. _summary example:\n",
    "\n",
    "The implementation looks complicated, but most of the code is boilerplate, and the\n",
    "actual logic is quite simple. You get a list of file names and paths to generate a\n",
    "summary for, define DataFrame columns and corresponding attributes, and then load the\n",
    "data one by one and extract the metadata. Let's see how the resulting summary looks\n",
    "like.\n",
    "\n",
    ".. note::\n",
    "\n",
    "    - If `ipywidgets <https://github.com/jupyter-widgets/ipywidgets>`_ is not installed, only the DataFrame will be displayed.\n",
    "    - If you are viewing this documentation online, the summary will not be interactive. Run the code locally to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erlab.io.set_loader(\"example_complete\")\n",
    "erlab.io.summarize()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Each cell in the summary table is formatted with :meth:`formatter\n",
    "<erlab.io.dataloader.LoaderBase.formatter>`. If additional formatting that cannot be\n",
    "achieved within :meth:`generate_summary\n",
    "<erlab.io.dataloader.LoaderBase.generate_summary>` is needed, :meth:`formatter\n",
    "<erlab.io.dataloader.LoaderBase.formatter>` can be inherited in the subclass.\n",
    "\n",
    "Tips\n",
    "~~~~\n",
    "\n",
    "- The data loading framework is designed to be simple and flexible, but it may not cover\n",
    "  all possible setups. If you encounter a setup that cannot be loaded with the existing\n",
    "  loaders, please let us know by opening an issue!\n",
    "\n",
    "- Before implementing a loader, see :doc:`../generated/erlab.io.dataloader` for\n",
    "  descriptions about each attribute, and the values and types of the expected outputs.\n",
    "  The implementation of existing loaders in the :mod:`erlab.io.plugins` module is a good\n",
    "  starting point; see the `source code on github\n",
    "  <https://github.com/kmnhan/erlabpy/tree/main/src/erlab/io/plugins>`_.\n",
    "\n",
    "- If you have implemented a new loader or have improved an existing one, consider\n",
    "  contributing it to the ERLabPy project by opening a pull request. We are always\n",
    "  looking for new loaders to support more experimental setups! See more about\n",
    "  contributing in the :doc:`../contributing`.\n",
    "\n",
    "- If you wish to add post-processing steps that are applicable to all data loaded by\n",
    "  that loader such as fixing the sign of the binding energy coordinates, you can inherit\n",
    "  the :meth:`post_process <erlab.io.dataloader.LoaderBase.post_process>` which by\n",
    "  default handles coordinate and attribute renaming. This method is called after the\n",
    "  data is loaded and can be used to modify the data before it is returned.\n",
    "\n",
    "- For complex data structures, constructing a full path from just the sequence number \n",
    "  and the data directory can be difficult. In this case, the :meth:`identify <erlab.io.\n",
    "  dataloader.LoaderBase.identify>` can be implemented to take additional keyword\n",
    "  arguments. All keyword arguments passed to :meth:`load\n",
    "  <erlab.io.dataloader.LoaderBase. load>` are passed to :meth:`identify\n",
    "  <erlab.io.dataloader.LoaderBase.identify>`!\n",
    "\n",
    "  For instance, consider data with different prefixes like ``A_001.h5``, ``A_002.h5``,\n",
    "  ``B_001.h5``, etc. stored in the same directory. The sequence number alone is not\n",
    "  enough to construct the full path. In this case, :meth:`identify\n",
    "  <erlab.io.dataloader.LoaderBase.identify>` can be implemented to take an additional\n",
    "  ``prefix`` argument which eliminates the ambiguity. Then, ``A_001.h5`` can be loaded\n",
    "  with ``erlab.io.load(1, prefix=\"A\")``.\n",
    "\n",
    "  If there are multiple file scans in this setup like ``A_001_S001.h5``,\n",
    "  ``A_001_S002.h5``, etc., we would want to pass the ``prefix`` parameter to :meth:`load\n",
    "  <erlab.io.dataloader.LoaderBase.load>` from an identifier given as a file name. This\n",
    "  is where the second return value of :meth:`infer_index\n",
    "  <erlab.io.dataloader.LoaderBase.infer_index>` comes in handy, where you can return a\n",
    "  dictionary which is passed to :meth:`load <erlab.io.dataloader.LoaderBase.load>`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "text/restructuredtext",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
