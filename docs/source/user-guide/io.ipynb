{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading & writing data\n",
    "\n",
    "In ERLabPy, data are represented as {class}`xarray.DataArray`, {class}`xarray.Dataset`, and {class}`xarray.DataTree` objects.\n",
    "\n",
    "- {class}`xarray.DataArray` are similar to waves in Igor Pro, but are much more flexible. Opposed to the maximum of 4 dimensions in Igor, {class}`xarray.DataArray` can have as many dimensions as you want (up to 64). Another advantage is that the coordinates of the dimensions do not have to be evenly spaced. In fact, they are not limited to numbers but can be any type of data, such as date and time representations.\n",
    "\n",
    "- {class}`xarray.Dataset` is a collection of {class}`xarray.DataArray` objects. It is used to store multiple data arrays that are related to each other, such as a set of measurements.\n",
    "\n",
    "- {class}`xarray.DataTree` is a hierarchical data structure that can store multiple {class}`xarray.Dataset` objects, just like an Igor experiment file with multiple waves within nested folders.\n",
    "\n",
    "See [Data Structures](https://docs.xarray.dev/en/latest/user-guide/data-structures.html) in the xarray documentation for a general introduction to xarray data structures.\n",
    "\n",
    "This guide will introduce you to reading and writing data from and to various file formats, and how to implement a custom plugin for a experimental setup.\n",
    "\n",
    ":::{note}\n",
    "\n",
    "If you are not familiar with {mod}`xarray`, it is recommended to read the [xarray tutorial](https://tutorial.xarray.dev/) and the [xarray user guide](https://docs.xarray.dev/en/stable/user-guide/index.html) first.\n",
    "\n",
    ":::\n",
    "\n",
    "Skip to the [corresponding section](loading-arpes-data) for guides on loading ARPES data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data with `xarray`\n",
    "\n",
    "{mod}`xarray` provides basic support for reading and writing NetCDF and HDF5 files into {mod}`xarray` objects. See the {mod}`xarray` documentation on [I/O operations ](https://docs.xarray.dev/en/stable/user-guide/io.html) for more information.\n",
    "\n",
    "Here, we will focus on working with data exported from Igor Pro and other commonly used file formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Igor Pro\n",
    "\n",
    "Installing ERLabPy automatically registers a backend for xarray that allows reading `.pxt`, `.pxp`, and `.ibw` files. This means that you can load these files directly into xarray using {func}`xarray.open_dataset` or {func}`xarray.open_dataarray` as if they were NetCDF files.\n",
    "\n",
    "In most cases, xarray will automatically detect the file format. For example, to load an `.ibw` file into a {class}`xarray.DataArray`, use the following code:\n",
    "\n",
    "```python\n",
    "import xarray as xr\n",
    "\n",
    "data = xr.open_dataarray(\"path/to/wave.ibw\")\n",
    "```\n",
    "\n",
    "Loading an experiment file to a {class}`xarray.DataTree` is also possible:\n",
    "\n",
    "```python\n",
    "data = xr.open_datatree(\"path/to/experiment.pxp\")\n",
    "```\n",
    "\n",
    "Along with the Igor Pro file formats, the backend also supports loading HDF5 files exported from Igor Pro. For such files, the engine must be specified explicitly with `engine=\"erlab-igor\"`.\n",
    "\n",
    ":::{warning}\n",
    "\n",
    "Loading waves from complex ``.pxp`` files may fail or produce unexpected results. It is recommended to export the waves to a ``.ibw`` file to load them in ERLabPy. If you encounter any problems, please let us know by opening an issue.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From arbitrary formats\n",
    "\n",
    "There are many python libraries that can read and write data in various formats. Here,\n",
    "some common file formats and how to read them are listed:\n",
    "\n",
    "* Spreadsheet data can be read using {func}`pandas.read_csv` and {func}`pandas.read_excel`.\n",
    "  \n",
    "  The resulting DataFrame can be converted to an xarray object using {meth}`pandas.DataFrame.to_xarray` or {meth}`xarray.Dataset.from_dataframe`.\n",
    "\n",
    "* When reading HDF5 files with arbitrary groups and metadata, you must first explore the  group structure using [h5netcdf](https://h5netcdf.org/). More conveniently, you can use {func}`xarray.open_groups` to inspect the group structure.\n",
    "\n",
    "* FITS files can be read with [astropy](https://docs.astropy.org/en/stable/io/fits/index.html).\n",
    "\n",
    "  In the near future, ERLabPy will provide a loader for FITS files.\n",
    "\n",
    "* For working with NeXus files, see {mod}`erlab.io.nexusutils`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing ``xarray`` objects to a file\n",
    "\n",
    "Since the state and variables of a Python interpreter are not saved, it is important to save your data in a format that can be easily read and written.\n",
    "\n",
    "While it is possible to save and load entire Python interpreter sessions using [pickle](https://docs.python.org/3/library/pickle.html) or the more versatile [dill](https://github.com/uqfoundation/dill), it is out of the scope of this guide. Instead, we recommend saving your data in a format that is easy to read, write, and share, such as HDF5 or NetCDF. To save and load xarray objects to such formats, see the xarray documentation on [I/O operations](https://docs.xarray.dev/en/stable/user-guide/io.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Igor Pro\n",
    "\n",
    "As an experimental feature, {func}`save_as_hdf5 <erlab.io.save_as_hdf5>` can save certain DataArrays in a format that is compatible with the Igor Pro HDF5 loader. An [accompanying Igor procedure ](https://github.com/kmnhan/erlabpy/blob/main/PythonInterface.ipf) is available in the repository. If loading in Igor Pro fails, try saving again with all attributes removed.\n",
    "\n",
    "Alternatively, [igorwriter](https://github.com/t-onoz/igorwriter) can be used to write numpy arrays to ``.ibw`` and ``.itx`` files directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(loading-arpes-data)=\n",
    "## ARPES data\n",
    "\n",
    "ARPES data from synchrotron endstations and laboratory setups worldwide are saved in diverse formats. ERLabPy’s data loading framework strives to offer a unified interface for loading ARPES data from various sources.\n",
    "\n",
    "To ensure seamless integration with common analysis procedures like momentum conversion and Fermi edge fitting, the data loaded into xarray objects must adhere to specific conventions.\n",
    "\n",
    "\n",
    "(data-conventions)=\n",
    "\n",
    "### Conventions\n",
    "\n",
    ":::{note}\n",
    "\n",
    "These conventions are not strictly enforced, but adhering to them will simplify the use of the provided analysis tools.\n",
    "\n",
    "Generally, any type of xarray object will be compatible with analysis routines that aren’t specific to ARPES, such as plotting, masking, transformations, curve fitting, interpolation, and so on.\n",
    "\n",
    ":::\n",
    "\n",
    "These are some rules that loaded ARPES data must follow to ensure compatibility with analysis procedures such as momentum conversion and fermi edge fitting:\n",
    "\n",
    "- Information about the experimental geometry is stored in the `'configuration'` attribute as an integer from 1 to 4. See [Nomenclature](nomenclature) and {class}`AxesConfiguration <erlab.constants.AxesConfiguration>` for more information.\n",
    "\n",
    "- Angles are stored in coordinates that are named according to the conventions in [Nomenclature](nomenclature).\n",
    "\n",
    "- The energy (binding or kinetic) is stored in a coordinate named `'eV'`. The sign of binding energies should be negative for occupied states.\n",
    "\n",
    "- The photon energy must be stored in a coordinate named `'hv'`.\n",
    "\n",
    "- The sample temperature, if available, is stored in an attribute or coordinate named `'sample_temp'`.\n",
    "\n",
    "- The work function of the system, if available, is stored in an attribute named `'sample_workfunction'`.\n",
    "\n",
    "- The angular resolution of the experiment, if available, is stored in an attribute named `'angle_resolution'`. This is only used to estimate momentum grid sizes when converting to momentum space.\n",
    "\n",
    "\n",
    "\n",
    "In addition, the following units are used:\n",
    "\n",
    "| Quantity         | Unit            |\n",
    "|:----------------:|:---------------:|\n",
    "| Energy           | eV              |\n",
    "| Angle            | deg             |\n",
    "| Temperature      | K               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading\n",
    "\n",
    "ERLabPy's data loading framework consists of various plugins, or *loaders*, each\n",
    "designed to load data from a different beamline or laboratory. Each *loader* is a class\n",
    "instance that has a `load` method which takes a file path or sequence number and returns\n",
    "data.\n",
    "\n",
    "Let's see the list of available loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "notebookRunGroups": {
     "groupValue": ""
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import erlab\n",
    "\n",
    "erlab.io.loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = [\"svg\", \"pdf\"]\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 96\n",
    "plt.rcParams[\"image.cmap\"] = \"viridis\"\n",
    "\n",
    "xr.set_options(display_expand_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access each loader using its name as an attribute or an item. For example, to\n",
    "access the loader for the ALS beamline 4.0.3 (MERLIN), you can use any of the following\n",
    "methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "erlab.io.loaders[\"merlin\"]\n",
    "erlab.io.loaders.merlin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading is done by calling the {meth}`load <erlab.io.dataloader.LoaderBase.load>` method of the loader. It requires an `identifier` parameter, which can be a path to a file or a sequence number. It also accepts a `data_dir` parameter, which specifies the directory where the data is stored.\n",
    "\n",
    "- If `identifier` is a sequence number, `data_dir` must be provided.\n",
    "\n",
    "- If `identifier` is a string and `data_dir` is provided, the path is constructed by\n",
    "  joining `data_dir` and `identifier`.\n",
    "\n",
    "- If `identifier` is a string and `data_dir` is not provided, `identifier` should be a\n",
    "  valid path to a file.\n",
    "\n",
    "Suppose we have data from the ALS beamline 4.0.3 stored as `/path/to/data/f_001.pxt`, `/path/to/data/f_002.pxt`, etc. To load `f_001.pxt`, all three of the following are valid:\n",
    "\n",
    "```python\n",
    "loader = erlab.io.loaders[\"merlin\"]\n",
    "\n",
    "loader.load(\"/path/to/data/f_001.pxt\")\n",
    "loader.load(\"f_001.pxt\", data_dir=\"/path/to/data\")\n",
    "loader.load(1, data_dir=\"/path/to/data\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the default loader and data directory\n",
    "\n",
    "In practice, a loader and a single directory will be used repeatedly in a session to load different data from the same experiment.\n",
    "\n",
    "Instead of explicitly specifying the loader and directory each time, a default loader and data directory can be set with {func}`erlab.io.set_loader` and {func}`erlab.io.set_data_dir`. All subsequent calls to the shortcut function {func}`erlab.io.load` will use the specified loader and data directory.\n",
    "\n",
    "```python\n",
    "erlab.io.set_loader(\"merlin\")\n",
    "erlab.io.set_data_dir(\"/path/to/data\")\n",
    "data_1 = erlab.io.load(1)\n",
    "data_2 = erlab.io.load(2)\n",
    "```\n",
    "\n",
    "The loader and data directory can also be controlled with a context manager:\n",
    "\n",
    "```python\n",
    "with erlab.io.loader_context(\"merlin\", data_dir=\"/path/to/data\"):\n",
    "    data_1 = erlab.io.load(1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data across multiple files\n",
    "\n",
    "For setups like the ALS beamline 4.0.3, some scans are stored over multiple files like\n",
    "`f_003_S001.pxt`, `f_003_S002.pxt`, and so on. In this case, the loader will\n",
    "automatically concatenate all files in the same scan. For example, *all of the\n",
    "following* will return the same concatenated data:\n",
    "\n",
    "```python\n",
    "erlab.io.load(3)\n",
    "erlab.io.load(\"f_003_S001.pxt\")\n",
    "erlab.io.load(\"f_003_S002.pxt\")\n",
    "```\n",
    "\n",
    "If you want to cherry-pick a single file, you can pass ``single=True`` to {meth}`load\n",
    "<erlab.io.dataloader.LoaderBase.load>`:\n",
    "\n",
    "```python\n",
    "erlab.io.load(\"f_003_S001.pxt\", single=True)\n",
    "```\n",
    "\n",
    "If you don't want automatic concatenation to happen, you can suppress it with `combine=False`. The following code will return a list of DataArrays:\n",
    "```python\n",
    "erlab.io.load(3, combine=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling multiple data directories\n",
    "\n",
    "If you call {func}`erlab.io.set_loader` or {func}`erlab.io.set_data_dir` multiple times, the last call will override the previous ones. While this is useful for changing the loader or data directory, it makes data loading *dependent on execution order*. This may lead to unexpected behavior in notebooks.\n",
    "\n",
    "If you plan to use multiple loaders or data directories in the same session, it is recommended to use the context manager {func}`erlab.io.loader_context`:\n",
    "\n",
    "```python\n",
    "with erlab.io.loader_context(\"merlin\", data_dir=\"/path/to/data\"):\n",
    "    data = erlab.io.load(identifier)\n",
    "```\n",
    "\n",
    "It may also be convenient to define functions that set the loader and data directory and\n",
    "call {func}`erlab.io.load` with the appropriate arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing data\n",
    "\n",
    "Some supported loaders can generate a {class}`pandas.DataFrame` containing an overview of the data in a given directory. The generated summary can be viewed as a table with the {meth}`summarize <erlab.io.dataloader.LoaderBase.summarize>` method.\n",
    "\n",
    "If `ipywidgets` is installed, an interactive widget is also displayed. This is useful for quickly skimming through the data.\n",
    "\n",
    "Just like {meth}`load <erlab.io.dataloader.LoaderBase.load>`, {meth}`summarize <erlab.io.dataloader.LoaderBase.summarize>` can also be accessed with the shortcut function {func}`erlab.io.summarize`. For example, to display a summary of the data available in the directory `/path/to/data` using the `'merlin'` loader:\n",
    "\n",
    "```python\n",
    "erlab.io.set_loader(\"merlin\")\n",
    "erlab.io.summarize(\"/path/to/data\")\n",
    "```\n",
    "If the path is not specified, the current data directory is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what the generated summary looks like, see the [example below](summary example).\n",
    "\n",
    ":::{note}\n",
    "\n",
    "If the [ImageTool manager](imagetool-manager-guide) is running, the a button to open the data in ImageTool is shown in the interactive summary.\n",
    "\n",
    ":::\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Alternatively, a Qt-based GUI for browsing and loading data is also available. See {mod}`erlab.interactive.explorer` for more information.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(implementing-plugins)=\n",
    "## Implementing a data loader plugin \n",
    "\n",
    "Implementing a new loader plugin to support an ARPES setup can be done by subclassing {class}`LoaderBase <erlab.io.dataloader.LoaderBase>` and inheriting or overriding some of its methods and attributes. Any subclass of {class}`LoaderBase <erlab.io.dataloader.LoaderBase>` is automatically registered as a loader.\n",
    "\n",
    "At the bare minimum, a loader must override the {attr}`name <erlab.io.dataloader.LoaderBase.name>` attribute and the {meth}`load_single <erlab.io.dataloader.LoaderBase.load_single>` method. Other additional attributes and methods can be implemented to provide more functionality.\n",
    "\n",
    "Before we dive into the details, let's first understand the data loading flow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading flow\n",
    "\n",
    "The core method of a loader is the {meth}`load_single <erlab.io.dataloader.LoaderBase.load_single>` method, which is given a path to a single file and must return the data as an xarray object. In most cases, this will be a {class}`xarray.DataArray`. In cases where the data is more complex, e.g., multiple region scans with different axes, returning a {class}`xarray.Dataset` or {class}`xarray.DataTree` is also possible. In {meth}`load_single <erlab.io.dataloader.LoaderBase.load_single>`, post-processing steps such as renaming and reordering dimensions should not be included, as this can be handled automatically by setting some class attributes that we will discuss later.\n",
    "\n",
    "ARPES data files from a single experiment usually follow a fixed naming scheme, e.g., `file_0001.h5`, `file_0002.h5`, and so on. If the naming scheme is well-defined, it is possible to infer the file path from a sequence number so that the user can use the sequence number directly to load the data. This can be accomplished by implementing the {meth}`identify <erlab.io.dataloader.LoaderBase.identify>` method which should infer the full path to a data file given an integer sequence number(`identifier`) and the path to a folder(`data_dir`).\n",
    "\n",
    "The following flowchart shows the process of loading data from a single scan, given the path to the directory (`data_dir`) and the sequence number or file name (`identifier`):\n",
    "\n",
    "```{image} ../images/flowchart_single.pdf\n",
    ":align: center\n",
    ":alt: Flowchart for loading data from a single file\n",
    "```\n",
    "\n",
    "If only all data formats were as simple as this! Unfortunately, there are some setups where data that belongs to a single scan is saved over multiple files. In this case, the files will look like `file_0001_0001.h5`, `file_0001_0002.h5`, etc., and we can no longer uniquely identify a single file with a sequence number. For these kinds of setups, an additional method {meth}`infer_index <erlab.io.dataloader.LoaderBase.infer_index>` must be implemented. The following flowchart shows the process of loading data from multiple files:\n",
    "\n",
    "```{image} ../images/flowchart_multiple.pdf\n",
    ":align: center\n",
    ":alt: Flowchart for loading data from multiple files\n",
    "```\n",
    "\n",
    "In this case, the method {meth}`identify <erlab.io.dataloader.LoaderBase.identify>` should resolve *all* files that belong to the given sequence number, and return a *list* of file paths along with a dictionary of coordinates that are varied across the files. For example, if there are three files for a scan taken at three different `beta` angles, the method should return a list of three file paths and a dictionary with `'beta'` as the sole key and an array of length 3 containing the angle as the value. An empty dictionary should be returned if there are no varying coordinates.\n",
    "\n",
    "The method {meth}`infer_index <erlab.io.dataloader.LoaderBase.infer_index>` must infer the sequence number from a bare file name (without the extension and directory name). For example, given `file_0003_0123`, the method should infer `3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A minimal example\n",
    "\n",
    "Consider a setup that saves data into a `.csv` file named `data_0001.csv`, `data_0002.csv`, and so on. A simple implementation of a loader for the setup will look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from erlab.io.dataloader import LoaderBase\n",
    "\n",
    "\n",
    "class MyLoader(LoaderBase):\n",
    "    name = \"my_loader\"\n",
    "    description = \"Barebones loader for CSV files\"\n",
    "    extensions = {\".csv\"}\n",
    "    skip_validate = False\n",
    "    always_single = True\n",
    "\n",
    "    def identify(self, num, data_dir):\n",
    "        file = os.path.join(data_dir, f\"data_{str(num).zfill(4)}.csv\")\n",
    "        return [file], {}\n",
    "\n",
    "    def load_single(self, file_path, without_values=False):\n",
    "        return pd.read_csv(file_path).to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some class attributes and methods have been implemented. For a detailed explanation of each attribute and method, see the {class}`LoaderBase <erlab.io.dataloader.LoaderBase>` documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loader has been properly registered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "erlab.io.loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "erlab.io.loaders[\"my_loader\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loader can be used just like the built-in loaders:\n",
    "\n",
    "```python\n",
    "data = erlab.io.loaders.my_loader.load(1, data_dir=\"/path/to/data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling metadata\n",
    "\n",
    "Unlike the previous example, real ARPES data is more than just a simple array of numbers. It contains metadata such as the experimental geometry, sample temperature, and so on. It is important to store this metadata in the xarray object in a consistent manner as defined [here](data-conventions).\n",
    "\n",
    "To obtain a consistent representation of the data, data loaded by {meth}`load_single <erlab.io.dataloader.LoaderBase.load_single>` must be post-processed to adhere to the conventions. Typically, this involves manipulating coordinate and attribute names, which is automatically performed based on the following class attributes:\n",
    "\n",
    "- {attr}`name_map <erlab.io.dataloader.LoaderBase.name_map>`\n",
    "\n",
    "- {attr}`coordinate_attrs <erlab.io.dataloader.LoaderBase.coordinate_attrs>`\n",
    "\n",
    "- {attr}`average_attrs <erlab.io.dataloader.LoaderBase.average_attrs>`\n",
    "\n",
    "- {attr}`additional_attrs <erlab.io.dataloader.LoaderBase.additional_attrs>`\n",
    "\n",
    "- {attr}`overridden_attrs <erlab.io.dataloader.LoaderBase.overridden_attrs>`\n",
    "\n",
    "- {attr}`additional_coords <erlab.io.dataloader.LoaderBase.additional_coords>`\n",
    "\n",
    "- {attr}`overridden_coords <erlab.io.dataloader.LoaderBase.overridden_coords>`\n",
    "\n",
    "Any post-processing steps that reach beyond renaming and reordering dimensions can be implemented in the {meth}`post_process <erlab.io.dataloader.LoaderBase.post_process>` method:\n",
    "\n",
    "```python\n",
    "def post_process(self, data: xr.DataArray) -> xr.DataArray:\n",
    "    data = super().post_process(data)\n",
    "    # Perform additional post-processing steps here\n",
    "    return data\n",
    "```\n",
    "\n",
    "The loaders perform a basic check for some of the [conventions](data-conventions) using {meth}`validate <erlab.io.dataloader.LoaderBase.validate>` for every data file loaded. A warning is issued if some are missing. This behavior can be controlled with loader class attributes {attr}`skip_validate <erlab.io.dataloader.LoaderBase.skip_validate>` and {attr}`strict_validation <erlab.io.dataloader.LoaderBase.strict_validation>`.\n",
    "\n",
    "### Data spanning multiple files\n",
    "\n",
    "Next, let's try to write a more realistic loader for a hypothetical setup that saves data as HDF5 files with the following naming scheme: `data_001.h5`, `data_002.h5`, and so on, with multiple scans named like `data_001_S001.h5`, `data_001_S002.h5`, etc. with the scan axis information stored in a separate file named `data_001_axis.csv`.\n",
    "\n",
    "Let us first generate a data directory and place some synthetic data in it. Before saving, we rename and set some attributes that resemble real ARPES data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import erlab\n",
    "from erlab.io.exampledata import generate_data_angles\n",
    "\n",
    "\n",
    "def make_data(beta=5.0, temp=20.0, hv=50.0, bandshift=0.0):\n",
    "    data = generate_data_angles(\n",
    "        shape=(250, 1, 300),\n",
    "        angrange={\"alpha\": (-15, 15), \"beta\": (beta, beta)},\n",
    "        hv=hv,\n",
    "        configuration=1,\n",
    "        temp=temp,\n",
    "        bandshift=bandshift,\n",
    "        assign_attributes=False,\n",
    "        seed=1,\n",
    "    ).T\n",
    "\n",
    "    # Rename coordinates. The loader must rename them back to the original names.\n",
    "    data = data.rename(\n",
    "        {\n",
    "            \"alpha\": \"ThetaX\",\n",
    "            \"beta\": \"Polar\",\n",
    "            \"eV\": \"BindingEnergy\",\n",
    "            \"hv\": \"PhotonEnergy\",\n",
    "            \"xi\": \"Tilt\",\n",
    "            \"delta\": \"Azimuth\",\n",
    "        }\n",
    "    )\n",
    "    dt = datetime.datetime.now()\n",
    "\n",
    "    # Assign some attributes that real data would have\n",
    "    data = data.assign_attrs(\n",
    "        {\n",
    "            \"LensMode\": \"Angular30\",  # Lens mode of the analyzer\n",
    "            \"SpectrumType\": \"Fixed\",  # Acquisition mode of the analyzer\n",
    "            \"PassEnergy\": 10,  # Pass energy of the analyzer\n",
    "            \"UndPol\": 0,  # Undulator polarization\n",
    "            \"Date\": dt.strftime(r\"%d/%m/%Y\"),  # Date of the measurement\n",
    "            \"Time\": dt.strftime(\"%I:%M:%S %p\"),  # Time of the measurement\n",
    "            \"TB\": temp,\n",
    "            \"X\": 0.0,\n",
    "            \"Y\": 0.0,\n",
    "            \"Z\": 0.0,\n",
    "        }\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "# Create a temporary directory\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "# Define coordinates for the scan\n",
    "beta_coords = np.linspace(2, 7, 10)\n",
    "\n",
    "# Generate and save cuts with different beta values\n",
    "for i, beta in enumerate(beta_coords):\n",
    "    data = make_data(beta=beta, temp=20.0, hv=50.0)\n",
    "    filename = f\"{tmp_dir.name}/data_001_S{str(i + 1).zfill(3)}.h5\"\n",
    "    data.to_netcdf(filename, engine=\"h5netcdf\")\n",
    "\n",
    "# Write scan coordinates to a csv file\n",
    "with open(f\"{tmp_dir.name}/data_001_axis.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Index\", \"Polar\"])\n",
    "\n",
    "    for i, beta in enumerate(beta_coords):\n",
    "        writer.writerow([i + 1, beta])\n",
    "\n",
    "# Generate some cuts with different band shifts\n",
    "for i in range(4):\n",
    "    data = make_data(beta=5.0, temp=20.0, hv=50.0, bandshift=-i * 0.05)\n",
    "    filename = f\"{tmp_dir.name}/data_{str(i + 2).zfill(3)}.h5\"\n",
    "    data.to_netcdf(filename, engine=\"h5netcdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have generated a folder that resembles typical data from an ARPES experiment. Let's list the contents of the folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(os.listdir(tmp_dir.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each HDF5 file represents a single ARPES cut. `data_001_S001.h5` to `data_001_S010.h5`\n",
    "represents an ARPES map with 10 cuts, with the scan axis recorded in\n",
    "`data_001_axis.csv`. Let's check what the raw data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.load_dataarray(f\"{tmp_dir.name}/data_002.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been properly loaded, but the coordinates and attributes have names that\n",
    "are specific to the beamline.\n",
    "\n",
    "Our loader should do three things: rename the coordinates and attributes to standard\n",
    "names, add metadata to the dataset, and combine related cuts into a single DataArray\n",
    "that contains the ARPES mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "Here, we easily loaded the data into an xarray object directly, but that is not the case for most experimental setups. Properly loading raw data into an xarray object is a complex process that requires knowledge of the data format and the experimental setup, and this is what must be implemented in the {meth}`load_single <erlab.io.dataloader.LoaderBase.load_single>`.\n",
    "\n",
    "ERLabPy provides convenient functions to ease this process. See [implementations of existing data loaders](https://github.com/kmnhan/erlabpy/tree/main/src/erlab/io/plugins) for examples.\n",
    "\n",
    ":::\n",
    "\n",
    "Now that we have the data, let's implement the loader. The biggest difference from the previous example is that we need to handle multiple files for a single scan in {meth}`identify <erlab.io.dataloader.LoaderBase.identify>`. Also, we have to implement {meth}`infer_index <erlab.io.dataloader.LoaderBase.infer_index>` to extract the scan number from the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import re\n",
    "\n",
    "import erlab\n",
    "\n",
    "\n",
    "class ExampleLoader(erlab.io.dataloader.LoaderBase):\n",
    "    name = \"example\"\n",
    "    description = \"Example loader for multiple files\"\n",
    "    extensions = {\".h5\"}\n",
    "\n",
    "    name_map = {\n",
    "        \"eV\": \"BindingEnergy\",\n",
    "        \"alpha\": \"ThetaX\",\n",
    "        \"beta\": [\"Polar\", \"Polar Compens\"],\n",
    "        # Can have multiple names assigned to the same name\n",
    "        # If both are present in the data, a ValueError will be raised\n",
    "        \"delta\": \"Azimuth\",\n",
    "        \"xi\": \"Tilt\",\n",
    "        \"hv\": \"PhotonEnergy\",\n",
    "        \"polarization\": \"UndPol\",\n",
    "        \"sample_temp\": \"TB\",\n",
    "    }\n",
    "    # Map the names of the coordinates or attributes in the resulting data to the names\n",
    "    # present in the data returned by `load_single`. Note that the order of\n",
    "    # non-dimension coordinates in the output data will follow the order of the keys in\n",
    "    # this dictionary.\n",
    "\n",
    "    coordinate_attrs: tuple[str, ...] = (\n",
    "        \"beta\",\n",
    "        \"delta\",\n",
    "        \"xi\",\n",
    "        \"hv\",\n",
    "        \"X\",\n",
    "        \"Y\",\n",
    "        \"Z\",\n",
    "        \"polarization\",\n",
    "        \"photon_flux\",\n",
    "        \"sample_temp\",\n",
    "    )\n",
    "    # Attributes to be used as coordinates. Place all attributes that we don't want to\n",
    "    # lose when merging multiple file scans here.\n",
    "\n",
    "    additional_attrs = {\n",
    "        \"configuration\": 1,  # Experimental geometry. Required for momentum conversion\n",
    "        \"sample_workfunction\": 4.3,\n",
    "    }\n",
    "    # Any additional metadata you want to add to the data. Note that attributes defined\n",
    "    # here will not be transformed into coordinates. If you wish to promote some fixed\n",
    "    # attributes to coordinates, add them to additional_coords.\n",
    "\n",
    "    additional_coords = {}\n",
    "    # Additional non-dimension coordinates to be added to the data, for instance the\n",
    "    # photon energy for lab-based ARPES.\n",
    "\n",
    "    always_single = False\n",
    "\n",
    "    def identify(self, num, data_dir):\n",
    "        data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "        coord_dict = {}\n",
    "\n",
    "        # Look for scans with data_###_S###.h5, and sort them\n",
    "        files = sorted(data_dir.glob(f\"data_{str(num).zfill(3)}_S*.h5\"))\n",
    "\n",
    "        if len(files) == 0:\n",
    "            # If no files found, look for data_###.h5\n",
    "            files = sorted(data_dir.glob(f\"data_{str(num).zfill(3)}.h5\"))\n",
    "            if len(files) > 1:\n",
    "                # More than one file found with the same scan number, show warning\n",
    "                erlab.utils.misc.emit_user_level_warning(\n",
    "                    f\"Multiple files found for scan {num}, using {files[0]}\"\n",
    "                )\n",
    "                files = files[:1]\n",
    "        else:\n",
    "            # If files found, extract coordinate values from the filenames\n",
    "            axis_file = data_dir / f\"data_{str(num).zfill(3)}_axis.csv\"\n",
    "            with axis_file.open(\"r\") as f:\n",
    "                header = f.readline().strip().split(\",\")\n",
    "\n",
    "            # Load the coordinates from the csv file\n",
    "            coord_arr = np.loadtxt(axis_file, delimiter=\",\", skiprows=1)\n",
    "\n",
    "            # Each header entry will contain a dimension name\n",
    "            for i, hdr in enumerate(header[1:]):\n",
    "                coord_dict[hdr] = coord_arr[: len(files), i + 1].astype(np.float64)\n",
    "\n",
    "        if len(files) == 0:\n",
    "            # If no files found up to this point, return None\n",
    "            return None\n",
    "\n",
    "        return files, coord_dict\n",
    "\n",
    "    def load_single(self, file_path, without_values=False):\n",
    "        return xr.open_dataarray(file_path, engine=\"h5netcdf\")\n",
    "\n",
    "    def infer_index(self, name):\n",
    "        # Get the scan number from file name\n",
    "        try:\n",
    "            scan_num: str = re.match(r\".*?(\\d{3})(?:_S\\d{3})?\", name).group(1)\n",
    "        except (AttributeError, IndexError):\n",
    "            return None, None\n",
    "\n",
    "        if scan_num.isdigit():\n",
    "            # The second return value, a dictionary, is reserved for more complex\n",
    "            # setups. See tips below for a brief explanation.\n",
    "            return int(scan_num), {}\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erlab.io.loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `example` loader has been registered. Let's test the loader by\n",
    "loading and plotting some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erlab.io.set_loader(\"example\")\n",
    "erlab.io.set_data_dir(tmp_dir.name)\n",
    "erlab.io.load(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erlab.io.load(5).qplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brilliant! We now have a working loader for our hypothetical setup. \n",
    "\n",
    ":::{note}\n",
    "\n",
    "- There are more class attributes and methods that can be inherited or overridden to customize the loader's behavior.\n",
    "\n",
    "- For single-file loaders which save data in well-known formats such as outputs from Scienta Omicron DA30 analyzers, SES, or NeXus, the implementation can be much more straightforward. See the implementations of existing data loaders for examples.\n",
    "\n",
    ":::\n",
    "\n",
    "However, in order to use {func}`erlab.io.summarize` with our loader, a few more methods and attributes need to be implemented. These are discussed in the next section.\n",
    "\n",
    "### Summary generation\n",
    "\n",
    "To enable summary generation, we need to implement two attributes and one method:\n",
    "\n",
    "- {attr}`formatters <erlab.io.dataloader.LoaderBase.formatters>`: A dictionary that maps attribute or coordinate names in the data to functions that convert the coordinate or attribute value into a human-readable form.\n",
    "\n",
    "- {attr}`summary_attrs <erlab.io.dataloader.LoaderBase.summary_attrs>`: A dictionary that maps summary column names to attribute or coordinate names in the data. A callable can also be used to generate entries for attributes that are not directly present in the data. \n",
    "\n",
    "- {meth}`files_for_summary <erlab.io.dataloader.LoaderBase.files_for_summary>`: A method that takes a path to a directory and returns a list of file paths in the directory that are associated with the loader. \n",
    "\n",
    "You can also choose to implement the following attribute to further customize the\n",
    "summary:\n",
    "\n",
    "- {attr}`summary_sort <erlab.io.dataloader.LoaderBase.summary_sort>`: A string that determines the column name to sort the summary table with.\n",
    "\n",
    "  If not provided, the table will respect the order of the files returned by {meth}`files_for_summary <erlab.io.dataloader.LoaderBase.files_for_summary>`.\n",
    "\n",
    "To improve the performance of summary generation, you can optionally implement {meth}`load_single <erlab.io.dataloader.LoaderBase.load_single>` to utilize the `without_values` argument. If it is True, it means that the values in the returned data of {meth}`load_single <erlab.io.dataloader.LoaderBase.load_single>` will not be accessed, so you can return the data with its values set to arbitrary numbers. This is useful when only the metadata is needed for the summary. An example of this will be shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_polarization(val) -> str:\n",
    "    val = round(float(val))\n",
    "    return {0: \"LH\", 2: \"LV\", -1: \"RC\", 1: \"LC\"}.get(val, str(val))\n",
    "\n",
    "\n",
    "def _parse_time(darr: xr.DataArray) -> datetime.datetime:\n",
    "    return datetime.datetime.strptime(\n",
    "        f\"{darr.attrs['Date']} {darr.attrs['Time']}\", \"%d/%m/%Y %I:%M:%S %p\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _determine_kind(darr: xr.DataArray) -> str:\n",
    "    data_type = \"xps\"\n",
    "    if \"alpha\" in darr.dims:\n",
    "        data_type = \"cut\"\n",
    "    if \"beta\" in darr.dims:\n",
    "        data_type = \"map\"\n",
    "    if \"hv\" in darr.dims:\n",
    "        data_type = \"hvdep\"\n",
    "    return data_type\n",
    "\n",
    "\n",
    "class ExampleLoaderComplete(ExampleLoader):\n",
    "    name = \"example_complete\"\n",
    "    description = \"Example loader that supports summary generation\"\n",
    "\n",
    "    formatters = {\n",
    "        \"polarization\": _format_polarization,\n",
    "        \"LensMode\": lambda x: x.replace(\"Angular\", \"A\"),\n",
    "    }\n",
    "\n",
    "    summary_attrs = {\n",
    "        \"Time\": _parse_time,\n",
    "        \"Type\": _determine_kind,\n",
    "        \"Lens Mode\": \"LensMode\",\n",
    "        \"Scan Type\": \"SpectrumType\",\n",
    "        \"T(K)\": \"sample_temp\",\n",
    "        \"Pass E\": \"PassEnergy\",\n",
    "        \"Polarization\": \"polarization\",\n",
    "        \"hv\": \"hv\",\n",
    "        \"x\": \"X\",\n",
    "        \"y\": \"Y\",\n",
    "        \"z\": \"Z\",\n",
    "        \"polar\": \"beta\",\n",
    "        \"tilt\": \"xi\",\n",
    "        \"azi\": \"delta\",\n",
    "    }\n",
    "\n",
    "    summary_sort = \"Time\"\n",
    "\n",
    "    def load_single(self, file_path, without_values=False):\n",
    "        darr = xr.open_dataarray(file_path, engine=\"h5netcdf\")\n",
    "\n",
    "        if without_values:\n",
    "            # Prevent loading values into memory\n",
    "            return xr.DataArray(\n",
    "                np.zeros(darr.shape, darr.dtype),\n",
    "                coords=darr.coords,\n",
    "                dims=darr.dims,\n",
    "                attrs=darr.attrs,\n",
    "                name=darr.name,\n",
    "            )\n",
    "\n",
    "        return darr\n",
    "\n",
    "    def files_for_summary(self, data_dir):\n",
    "        return erlab.io.utils.get_files(data_dir, extensions=[\".h5\"])\n",
    "\n",
    "\n",
    "erlab.io.loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(summary example)=\n",
    "\n",
    "Let's see how the resulting summary looks like.\n",
    "\n",
    ":::{note}\n",
    "\n",
    "- If [ipywidgets](https://github.com/jupyter-widgets/ipywidgets) is not installed, only the DataFrame will be displayed.\n",
    "- If you are viewing this documentation online, the summary will not be interactive. Run the code locally to try it out.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erlab.io.set_loader(\"example_complete\")\n",
    "erlab.io.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cell in the summary table is formatted with {meth}`formatter <erlab.io.dataloader.LoaderBase.formatter>` after applying the {attr}`formatters <erlab.io.dataloader.LoaderBase.formatters>`.\n",
    "\n",
    "### Tips\n",
    "\n",
    "- The data loading framework is designed to be simple and flexible, but it may not cover all possible setups. If you encounter a setup that cannot be loaded with the existing api, please let us know by opening an issue!\n",
    "\n",
    "- Before implementing a loader, see {mod}`erlab.io.dataloader` for descriptions about each attribute, and the values and types of the expected outputs. The implementation of existing loaders in the {mod}`erlab.io.plugins` module is a good starting point; see the [source code on github](https://github.com/kmnhan/erlabpy/tree/main/src/erlab/io/plugins).\n",
    "\n",
    "- If you wish to add general post-processing steps such as fixing the sign of the binding energy coordinates, you can reimplement {meth}`post_process <erlab.io.dataloader.LoaderBase.post_process>` which by default handles coordinate and attribute renaming.\n",
    "\n",
    "- For complex data structures, constructing a full path from just the sequence number  and the data directory can be difficult. In this case, the {meth}`identify <erlab.io. dataloader.LoaderBase.identify>` can be implemented to take additional keyword arguments. All additional keyword arguments passed to {meth}`load <erlab.io.dataloader.LoaderBase.load>` are passed to {meth}`identify <erlab.io.dataloader.LoaderBase.identify>`.\n",
    "\n",
    "  For instance, consider data with different prefixes like `A_001.h5`, `A_002.h5`, `B_001.h5`, etc. stored in the same directory. In this case, we can't uniquely infer the file path from the sequence number alone. In this case, {meth}`identify <erlab.io.dataloader.LoaderBase.identify>` can be implemented to take an additional `prefix` argument to eliminate the ambiguity, after which `A_001.h5` can be loaded with `erlab.io.load(1, prefix=\"A\")`.\n",
    "\n",
    "  If there are multiple file scans in this setup like `A_001_S001.h5`, `A_001_S002.h5`, etc., we would want to pass the `prefix` parameter to {meth}`load <erlab.io.dataloader.LoaderBase.load>` from an identifier given as a file name. This is where the second return value of {meth}`infer_index <erlab.io.dataloader.LoaderBase.infer_index>` comes in handy, where you can return a dictionary which is passed to {meth}`load <erlab.io.dataloader.LoaderBase.load>`.\n",
    "\n",
    "  For an example of this, see the implementation of {class}`erlab.io.plugins.erpes.ERPESLoader`.\n",
    "\n",
    "- If you have implemented a new loader or have improved an existing one, consider contributing it to the ERLabPy project by opening a pull request. We are always looking for new loaders to support more experimental setups! See more about contributing [here](../contributing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to cleanup the temporary directory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
